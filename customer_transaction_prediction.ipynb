{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree - Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Santander Customer Transaction Prediction\n",
    "\n",
    "This project's objective is to predict whether a customer will make a specific transaction in the future or not, regardless of the amount of money transacted. To achieve this objective we'll be using anonymized consisting of 200 features along with a target column for training, which states if the user made the transaction or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_5</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4763</td>\n",
       "      <td>-2.3182</td>\n",
       "      <td>12.6080</td>\n",
       "      <td>8.6264</td>\n",
       "      <td>10.9621</td>\n",
       "      <td>3.5609</td>\n",
       "      <td>4.5322</td>\n",
       "      <td>15.2255</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.3068</td>\n",
       "      <td>6.6025</td>\n",
       "      <td>5.2912</td>\n",
       "      <td>0.4403</td>\n",
       "      <td>14.9452</td>\n",
       "      <td>1.0314</td>\n",
       "      <td>-3.6241</td>\n",
       "      <td>9.7670</td>\n",
       "      <td>12.5809</td>\n",
       "      <td>-4.7602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_6</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8091</td>\n",
       "      <td>-0.0832</td>\n",
       "      <td>9.3494</td>\n",
       "      <td>4.2916</td>\n",
       "      <td>11.1355</td>\n",
       "      <td>-8.0198</td>\n",
       "      <td>6.1961</td>\n",
       "      <td>12.0771</td>\n",
       "      <td>...</td>\n",
       "      <td>8.7830</td>\n",
       "      <td>6.4521</td>\n",
       "      <td>3.5325</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>18.3314</td>\n",
       "      <td>0.5845</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>9.1143</td>\n",
       "      <td>10.8869</td>\n",
       "      <td>-3.2097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_7</td>\n",
       "      <td>0</td>\n",
       "      <td>13.5580</td>\n",
       "      <td>-7.9881</td>\n",
       "      <td>13.8776</td>\n",
       "      <td>7.5985</td>\n",
       "      <td>8.6543</td>\n",
       "      <td>0.8310</td>\n",
       "      <td>5.6890</td>\n",
       "      <td>22.3262</td>\n",
       "      <td>...</td>\n",
       "      <td>13.1700</td>\n",
       "      <td>6.5491</td>\n",
       "      <td>3.9906</td>\n",
       "      <td>5.8061</td>\n",
       "      <td>23.1407</td>\n",
       "      <td>-0.3776</td>\n",
       "      <td>4.2178</td>\n",
       "      <td>9.4237</td>\n",
       "      <td>8.6624</td>\n",
       "      <td>3.4806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_8</td>\n",
       "      <td>0</td>\n",
       "      <td>16.1071</td>\n",
       "      <td>2.4426</td>\n",
       "      <td>13.9307</td>\n",
       "      <td>5.6327</td>\n",
       "      <td>8.8014</td>\n",
       "      <td>6.1630</td>\n",
       "      <td>4.4514</td>\n",
       "      <td>10.1854</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4298</td>\n",
       "      <td>14.7510</td>\n",
       "      <td>1.6395</td>\n",
       "      <td>1.4181</td>\n",
       "      <td>14.8370</td>\n",
       "      <td>-1.9940</td>\n",
       "      <td>-1.0733</td>\n",
       "      <td>8.1975</td>\n",
       "      <td>19.5114</td>\n",
       "      <td>4.8453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_9</td>\n",
       "      <td>0</td>\n",
       "      <td>12.5088</td>\n",
       "      <td>1.9743</td>\n",
       "      <td>8.8960</td>\n",
       "      <td>5.4508</td>\n",
       "      <td>13.6043</td>\n",
       "      <td>-16.2859</td>\n",
       "      <td>6.0637</td>\n",
       "      <td>16.8410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>6.3160</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>3.6885</td>\n",
       "      <td>14.8344</td>\n",
       "      <td>0.4467</td>\n",
       "      <td>14.1287</td>\n",
       "      <td>7.9133</td>\n",
       "      <td>16.2375</td>\n",
       "      <td>14.2514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4    var_5  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607  -9.2834   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622   7.0433   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825  -9.0837   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846  -1.8361   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772   2.4486   \n",
       "5  train_5       0  11.4763 -2.3182  12.6080  8.6264  10.9621   3.5609   \n",
       "6  train_6       0  11.8091 -0.0832   9.3494  4.2916  11.1355  -8.0198   \n",
       "7  train_7       0  13.5580 -7.9881  13.8776  7.5985   8.6543   0.8310   \n",
       "8  train_8       0  16.1071  2.4426  13.9307  5.6327   8.8014   6.1630   \n",
       "9  train_9       0  12.5088  1.9743   8.8960  5.4508  13.6043 -16.2859   \n",
       "\n",
       "    var_6    var_7   ...     var_190  var_191  var_192  var_193  var_194  \\\n",
       "0  5.1187  18.6266   ...      4.4354   3.9642   3.1364   1.6910  18.5227   \n",
       "1  5.6208  16.5338   ...      7.6421   7.7214   2.5837  10.9516  15.4305   \n",
       "2  6.9427  14.6155   ...      2.9057   9.7905   1.6704   1.6858  21.6042   \n",
       "3  5.8428  14.9250   ...      4.4666   4.7433   0.7178   1.4214  23.0347   \n",
       "4  5.9405  19.2514   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876   \n",
       "5  4.5322  15.2255   ...     -6.3068   6.6025   5.2912   0.4403  14.9452   \n",
       "6  6.1961  12.0771   ...      8.7830   6.4521   3.5325   0.1777  18.3314   \n",
       "7  5.6890  22.3262   ...     13.1700   6.5491   3.9906   5.8061  23.1407   \n",
       "8  4.4514  10.1854   ...      1.4298  14.7510   1.6395   1.4181  14.8370   \n",
       "9  6.0637  16.8410   ...      0.5543   6.3160   1.0371   3.6885  14.8344   \n",
       "\n",
       "   var_195  var_196  var_197  var_198  var_199  \n",
       "0  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "2   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "5   1.0314  -3.6241   9.7670  12.5809  -4.7602  \n",
       "6   0.5845   9.1104   9.1143  10.8869  -3.2097  \n",
       "7  -0.3776   4.2178   9.4237   8.6624   3.4806  \n",
       "8  -1.9940  -1.0733   8.1975  19.5114   4.8453  \n",
       "9   0.4467  14.1287   7.9133  16.2375  14.2514  \n",
       "\n",
       "[10 rows x 202 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing main libraries for the project\n",
    "# Other eventual libraries will be imported on demand\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display # to use display() for DataFrames\n",
    "\n",
    "\n",
    "# Loading training and testing data\n",
    "training_data = pd.read_csv(\"train.csv\")\n",
    "testing_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Loading sample submission data\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Peeking at first 10 observations\n",
    "display(training_data.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of customers: 200000\n",
      "Customers that will not make future transactions: 179902\n",
      "Customers that will make future transactions: 20098\n",
      "Percentage of customers tha will make future transactions: 10.049%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of customers\n",
    "n_cust = training_data.shape[0]\n",
    "\n",
    "# Number of customers that will not make future transactions\n",
    "n_no_fut_trans = training_data[training_data[\"target\"] == 0].shape[0]\n",
    "\n",
    "# Number of customers that will make future transactions\n",
    "n_fut_trans = training_data[training_data[\"target\"] == 1].shape[0]\n",
    "\n",
    "positions = [0.5, 1.5]\n",
    "customers = [n_fut_trans, n_no_fut_trans]\n",
    "labels = ['Future transactions', 'No future transactions' ]\n",
    "\n",
    "plt.bar(positions, customers, align='center', alpha=0.9)\n",
    "plt.xticks(positions, labels)\n",
    "plt.ylabel('Amount')\n",
    "plt.title('Customers Distribution')\n",
    "\n",
    "print(\"Total number of customers: {}\".format(n_cust))\n",
    "print(\"Customers that will not make future transactions: {}\".format(n_no_fut_trans))\n",
    "print(\"Customers that will make future transactions: {}\".format(n_fut_trans))\n",
    "print(\"Percentage of customers tha will make future transactions: {}%\".format(n_fut_trans/n_cust*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points considered outliers in more than one feature: 1556\n",
      "Number of points considered outliers in some feature: 24896\n"
     ]
    }
   ],
   "source": [
    "# Outlier detection (Turkey's method)\n",
    "outliers_t  = []\n",
    "repeated = []\n",
    "\n",
    "# Features\n",
    "X = data.drop(['target'], axis = 1)\n",
    "\n",
    "# For each feature \n",
    "for feature in X.keys():\n",
    "    \n",
    "    # Calculate first quartile\n",
    "    Q1 = np.percentile(X[feature], 25)\n",
    "    \n",
    "    # Calculate\n",
    "    Q3 = np.percentile(X[feature], 75)\n",
    "    \n",
    "    # Calculate interquatile range * 1.5\n",
    "    step = (Q3-Q1)*1.5\n",
    "    \n",
    "    for i in list((X[~((X[feature] >= Q1 - step) & (X[feature] <= Q3 + step))]).index.values):\n",
    "        if i not in outliers_t:\n",
    "            outliers_t.append(i)\n",
    "        elif i not in repeated:\n",
    "            repeated.append(i)\n",
    "\n",
    "\n",
    "print(\"Number of points considered outliers in more than one feature: {}\".format(len(repeated)))\n",
    "print(\"Number of points considered outliers in some feature: {}\".format(len(outliers_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers that will not make future transactions: 178569\n",
      "Customers that will make future transactions: 19875\n",
      "Percentage of customers tha will make future transactions: 9.9375%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Dropping rows with outlier values in more than one feature\n",
    "filtered_data = data.drop(repeated)\n",
    "\n",
    "# Checking data distribution after outliers removal\n",
    "# Number of customers that will not make future transactions\n",
    "n_no_fut_trans = filtered_data[filtered_data[\"target\"] == 0].shape[0]\n",
    "# Number of customers that will make future transactions\n",
    "n_fut_trans = filtered_data[filtered_data[\"target\"] == 1].shape[0]\n",
    "\n",
    "print(\"Customers that will not make future transactions: {}\".format(n_no_fut_trans))\n",
    "print(\"Customers that will make future transactions: {}\".format(n_fut_trans))\n",
    "print(\"Percentage of customers tha will make future transactions: {}%\".format(n_fut_trans/n_cust*100))\n",
    "\n",
    "# Generating smote (oversampling) \n",
    "smt = SMOTE(random_state=42)\n",
    "\n",
    "# Generating StratifiedKFold Cross-validator\n",
    "n_folds = 10\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components after pca: 90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "filtered_features = filtered_data.drop('target', axis=1)\n",
    "filtered_targets= filtered_data[['target']].values\n",
    "\n",
    "# Applying pca to the filtered data\n",
    "pca = PCA(n_components=0.9, svd_solver='full').fit(filtered_features.reset_index(drop=True))\n",
    "\n",
    "# Checking number of components\n",
    "print(\"Number of components after pca: {}\".format(pca.n_components_))\n",
    "\n",
    "# Transforming data based on components\n",
    "transf_features = pca.transform(filtered_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection (Local Outlier Factor)\n",
    "# Not viable due to O(n²) complexity, takes too long as result of dataset size\n",
    "# Kept for future reference only\n",
    "\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# lof = LocalOutlierFactor()\n",
    "\n",
    "# outliers_l = lof.fit_predict(X)\n",
    "\n",
    "# print(\"Number of points considered outliers: {}\".format(len(outliers_l[outliers_l == -1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, fbeta_score\n",
    "from time import time\n",
    "\n",
    "# Function used for the classifiers pipeline\n",
    "def train_predict(classifier, X_train, y_train, X_test, y_test, sample_size=-1):\n",
    "   \n",
    "    results = {}\n",
    "    \n",
    "    if(sample_size == -1):\n",
    "        sample_size = X_train.shape[0]\n",
    "        \n",
    "    start = time() # Get training start time\n",
    "    classifier = classifier.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    end = time() # Get training end time\n",
    "    \n",
    "    # Storing training time\n",
    "    results['train_time'] = end-start\n",
    "    \n",
    "    start = time() # Get predictions start time\n",
    "    predictions_test = classifier.predict(X_test)\n",
    "    predictions_train = classifier.predict(X_train)\n",
    "    end = time() # Get predictions end time\n",
    "    \n",
    "    results['pred_time'] = end-start\n",
    "    \n",
    "    # Compute area under the receive operating characterist curve using roc_auc_acore\n",
    "    results['roc_train'] = roc_auc_score(y_train, predictions_train)\n",
    "    results['roc_test'] = roc_auc_score(y_test, predictions_test)\n",
    "    \n",
    "    # Compute F-score using fbeta_score\n",
    "    results['f_train'] = fbeta_score(y_train, predictions_train, beta=2)\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=2)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code provided by udacity from machine learning engineer nanodegree project \n",
    "def evaluate(results, roc_auc, f1):\n",
    "    \"\"\"\n",
    "    Visualization code to display results of various learners.\n",
    "    \n",
    "    inputs:\n",
    "      - learners: a list of supervised learners\n",
    "      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n",
    "      - accuracy: The score for the naive predictor\n",
    "      - f1: The score for the naive predictor\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(2, 4, figsize = (11,7))\n",
    "\n",
    "    # Constants\n",
    "    bar_width = 0.3\n",
    "    colors = ['#A00000','#00A0A0','#00A000']\n",
    "    \n",
    "    # Super loop to plot four panels of data\n",
    "    for k, learner in enumerate(results.keys()):\n",
    "        for j, metric in enumerate(['train_time', 'roc_train', 'f_train', 'pred_time', 'roc_test', 'f_test']):\n",
    "            for i in np.arange(3):\n",
    "                \n",
    "                # Creative plot code\n",
    "                ax[j//3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n",
    "                ax[j//3, j%3].set_xticks([0.45, 1.45, 2.45])\n",
    "                ax[j//3, j%3].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n",
    "                ax[j//3, j%3].set_xlabel(\"Training Set Size\")\n",
    "                ax[j//3, j%3].set_xlim((-0.1, 3.0))\n",
    "    \n",
    "    # Add unique y-labels\n",
    "    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[0, 1].set_ylabel(\"ROC AUC Score\")\n",
    "    ax[0, 2].set_ylabel(\"F-score\")\n",
    "    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[1, 1].set_ylabel(\"ROC AUC Score\")\n",
    "    ax[1, 2].set_ylabel(\"F-score\")\n",
    "    \n",
    "    # Add titles\n",
    "    ax[0, 0].set_title(\"Model Training\")\n",
    "    ax[0, 1].set_title(\"ROC AUC on Training Subset\")\n",
    "    ax[0, 2].set_title(\"F-score on Training Subset\")\n",
    "    ax[1, 0].set_title(\"Model Predicting\")\n",
    "    ax[1, 1].set_title(\"ROC AUC Score on Testing Set\")\n",
    "    ax[1, 2].set_title(\"F-score on Testing Set\")\n",
    "    \n",
    "    # Add horizontal lines for naive predictors\n",
    "    ax[0, 1].axhline(y = roc_auc, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[1, 1].axhline(y = roc_auc, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[0, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    ax[1, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n",
    "    \n",
    "    # Set y-limits for score panels\n",
    "    ax[0, 1].set_ylim((0, 1))\n",
    "    ax[0, 2].set_ylim((0, 1))\n",
    "    ax[1, 1].set_ylim((0, 1))\n",
    "    ax[1, 2].set_ylim((0, 1))\n",
    "\n",
    "    # Set additional plots invisibles\n",
    "    ax[0, 3].set_visible(False)\n",
    "    ax[1, 3].axis('off')\n",
    "\n",
    "    # Create legend\n",
    "    for i, learner in enumerate(results.keys()):\n",
    "        plt.bar(0, 0, color=colors[i], label=learner)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Aesthetics\n",
    "    plt.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def mean_result(results):\n",
    "    \n",
    "    final_results = defaultdict(dict)\n",
    "    \n",
    "    for clf, data in results.items():\n",
    "        classifier_data = data\n",
    "        \n",
    "        for size, values in classifier_data.items():\n",
    "            results = values\n",
    "            \n",
    "            train_time = 0.0\n",
    "            pred_time = 0.0\n",
    "            roc_train = 0.0\n",
    "            roc_test = 0.0\n",
    "            f_train = 0.0\n",
    "            f_test = 0.0\n",
    "            \n",
    "            for fold_result in results:\n",
    "                train_time += fold_result['train_time']\n",
    "                pred_time += fold_result['pred_time']\n",
    "                roc_train += fold_result['roc_train']\n",
    "                roc_test += fold_result['roc_test']\n",
    "                f_train += fold_result['f_train']\n",
    "                f_test += fold_result['f_test']\n",
    "            \n",
    "            train_time /= len(fold_result)\n",
    "            pred_time /= len(fold_result)\n",
    "            roc_train /= len(fold_result)\n",
    "            roc_test /= len(fold_result)\n",
    "            f_train /= len(fold_result)\n",
    "            f_test /= len(fold_result)\n",
    "            \n",
    "            result = {'train_time': train_time, 'pred_time': pred_time, \\\n",
    "                      'roc_train': roc_train, 'roc_test': roc_test, \\\n",
    "                      'f_train': f_train, 'f_test': f_test}\n",
    "            \n",
    "            final_results[clf][size] = result\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "clf_A = CatBoostClassifier(silent=True, eval_metric='AUC', early_stopping_rounds=500, \\\n",
    "                           task_type='GPU', od_type='Iter', random_state=42)\n",
    "clf_B = XGBClassifier()\n",
    "clf_C = GaussianNB()\n",
    "\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = defaultdict(lambda: [])\n",
    "    j = 0\n",
    "    for train_index, test_index in skf.split(transf_features, filtered_targets):\n",
    "\n",
    "        # Separating training and testing data\n",
    "        X_train, X_test = transf_features[train_index], transf_features[test_index]\n",
    "        y_train, y_test = filtered_targets[train_index], filtered_targets[test_index]\n",
    "\n",
    "        # Oversampling minor class\n",
    "        X_train, y_train = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "        size_1 = int(0.01 * X_train.shape[0])\n",
    "        size_10 =int(0.1 * X_train.shape[0])\n",
    "        size_100 = X_train.shape[0]\n",
    "\n",
    "        for i, samples in enumerate([size_1, size_10, size_100]):\n",
    "            results[clf_name][i].append(train_predict(clf, X_train, y_train, X_test, y_test, samples))\n",
    "        j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating results mean\n",
    "final_results = mean_result(results)\n",
    "\n",
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "evaluate(final_results, 0.8, 0.8)\n",
    "\n",
    "# Checking numeric values for each method used (complete training set)\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    print(\"{} \\n Time - Training: {} \\t Testing: {} \\n ROC AUC - Train: {} \\t Test: {} \\n F-Score: Train: {} \\t Test: {}\" \\\n",
    "          .format(clf_name, \\\n",
    "          final_results[clf_name][2]['train_time'],final_results[clf_name][2]['pred_time'], \\\n",
    "          final_results[clf_name][2]['roc_train'],final_results[clf_name][2]['roc_test'], \\\n",
    "          final_results[clf_name][2]['f_train'], final_results[clf_name][2]['f_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Training until validation scores don't improve for 2048 rounds.\n",
      "[2048]\ttraining's auc: 0.969931\tvalid_1's auc: 0.890834\n",
      "[4096]\ttraining's auc: 0.989464\tvalid_1's auc: 0.895319\n",
      "[6144]\ttraining's auc: 0.997329\tvalid_1's auc: 0.895559\n",
      "Early stopping, best iteration is:\n",
      "[4725]\ttraining's auc: 0.992844\tvalid_1's auc: 0.895886\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 2048 rounds.\n",
      "[2048]\ttraining's auc: 0.969556\tvalid_1's auc: 0.893836\n",
      "[4096]\ttraining's auc: 0.989441\tvalid_1's auc: 0.898408\n",
      "[6144]\ttraining's auc: 0.997279\tvalid_1's auc: 0.898366\n",
      "[8192]\ttraining's auc: 0.999478\tvalid_1's auc: 0.898304\n",
      "Early stopping, best iteration is:\n",
      "[6244]\ttraining's auc: 0.997475\tvalid_1's auc: 0.898539\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 2048 rounds.\n",
      "[2048]\ttraining's auc: 0.970255\tvalid_1's auc: 0.885461\n",
      "[4096]\ttraining's auc: 0.989541\tvalid_1's auc: 0.890219\n",
      "[6144]\ttraining's auc: 0.997321\tvalid_1's auc: 0.890491\n",
      "Early stopping, best iteration is:\n",
      "[4782]\ttraining's auc: 0.993201\tvalid_1's auc: 0.890902\n",
      "Fold 4\n",
      "Training until validation scores don't improve for 2048 rounds.\n",
      "[2048]\ttraining's auc: 0.969915\tvalid_1's auc: 0.894342\n",
      "[4096]\ttraining's auc: 0.989379\tvalid_1's auc: 0.898712\n",
      "[6144]\ttraining's auc: 0.997315\tvalid_1's auc: 0.898826\n",
      "[8192]\ttraining's auc: 0.999462\tvalid_1's auc: 0.89855\n",
      "Early stopping, best iteration is:\n",
      "[6440]\ttraining's auc: 0.997855\tvalid_1's auc: 0.89898\n",
      "Fold 5\n",
      "Training until validation scores don't improve for 2048 rounds.\n",
      "[2048]\ttraining's auc: 0.970391\tvalid_1's auc: 0.88712\n",
      "[4096]\ttraining's auc: 0.989662\tvalid_1's auc: 0.892385\n",
      "[6144]\ttraining's auc: 0.9973\tvalid_1's auc: 0.892856\n",
      "[8192]\ttraining's auc: 0.9994\tvalid_1's auc: 0.892955\n",
      "[10240]\ttraining's auc: 0.999845\tvalid_1's auc: 0.892946\n",
      "Early stopping, best iteration is:\n",
      "[8580]\ttraining's auc: 0.999552\tvalid_1's auc: 0.893059\n",
      "Overall train ROC AUC: 0.9961852559847093\n",
      "Overall test ROC AUC: 0.8954732519092634\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "submission_data = pd.read_csv('test.csv')\n",
    "\n",
    "pred_submission = 0\n",
    "\n",
    "submission_data.drop('ID_code', axis=1, inplace=True)\n",
    "\n",
    "results_train = np.empty((0,0))\n",
    "results_test = np.empty((0,0))\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(filtered_features, filtered_targets)):\n",
    "    \n",
    "        print(\"Fold {}\".format(index+1))\n",
    "        # Separating training and testing data\n",
    "        X_train, X_test = filtered_features.values[train_index], filtered_features.values[test_index]\n",
    "        y_train, y_test = filtered_targets[train_index], filtered_targets[test_index]\n",
    "\n",
    "        # Oversampling minor class\n",
    "#         X_train, y_train = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Creating dataset for classifier \n",
    "        train_data = lgb.Dataset(X_train, label=y_train[:,0])\n",
    "        test_data = lgb.Dataset(X_test, label=y_test[:,0])\n",
    "        \n",
    "        param = {'random_state':42, 'metric': 'auc', 'bagging_freq': 5, 'bagging_fraction': 0.35,\n",
    "                 'min_data_in_leaf': 256, 'learning_rate': 0.005, 'objective': 'binary', 'max_leaves': 80,\n",
    "                 'num_threads': 4, 'is_unbalance': True}\n",
    "        model = lgb.train(param, train_data, 100000, valid_sets = [train_data, test_data], early_stopping_rounds=2048,verbose_eval=2048)\n",
    "        \n",
    "        predictions_train = model.predict(X_train, num_iteration=model.best_iteration)\n",
    "        predictions_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        \n",
    "        train_auc = roc_auc_score(y_train, predictions_train)\n",
    "        test_auc = roc_auc_score(y_test, predictions_test)\n",
    "        \n",
    "        results_train = np.append(results_train, train_auc)\n",
    "        results_test = np.append(results_test, test_auc)\n",
    "        \n",
    "        pred_submission += model.predict(submission_data, num_iteration=model.best_iteration)/n_folds\n",
    "        \n",
    "print(\"Overall train ROC AUC: {}\".format(np.mean(results_train)))\n",
    "print(\"Overall test ROC AUC: {}\".format(np.mean(results_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = pred_submission\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
